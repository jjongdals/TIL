# 0713 오늘의 공부
- 시계열 데이터 및 인공신경망 공부

- Non-Sequential vs Sequential Data
1. Non-Sequential Data: 시간 정보를 포함하지 않고 생성하는 데이터
순차데이터가 아닌 경우 데이터는 N x D matrix로 표현가능

-> 이는 순서가 없는 인공신경망 구조를 가진다.


2. Sequential Data: 시간 정보를 포함하여 순차적으로 생성되는 데이터
순차데이터의 경우 N x T X D의 3차원 벡터로 표현된다.

-> 이는 순서가 있는 인공신경망 구조를 가진다. 즉, 은닉층이 여러 번 반복돼서 활용되는 구조를 가진다.

따라서, RNN의 구조를 가진다.

3. Vanilla RNN
이전 은닉 노드의 정보를 같이 활용하여 예측을 한다.

RNN에서 Backpropagation을 진행함(이유: 현재 입력과 출력 간의 쌍들을 잘 맞추는 가중치 행렬을 구하기 위해)

cost와 출력,은닉노드 간의 가중치를 편미분해서 진행한다.

일반적으로 식을 정리하면, 1 - tanh(x)^2가 곱해지는데, 이는 값이 클수록 gradient 값이 0이 되기 때문에 오래 전의 값은 반영하지 못하고 학습을 진행하지 못한다는 문제가 생긴다.

따라서, 이를 해결하기 위한 구조가 LSTM과 GRU 구조다.

4. LSTM
LSTM은 cell state도 추가하여 여러 단계를 거치는데,

4-1. Cell state의 정보를 얼마만큼 망각할 것인지 결정 
Forget gate: 이전의 hidden state와 현 단계의 입력으로부터 0-1 사이의 값을 출력한다. (0은 cell state에 저장된 정보를 무시하고 1은 cell state에 저장된 정보를 보존한다. 이는 이전의 정보가 유용했는지 유무를 판단할 수 있다.)

4-2. 새로운 정보를 얼마만큼 cell state에 저장할 것인지 결정
Input gate: 어떤 값을 업데이트할 것인지 결정한다.

4-3. 이전 cell state를 새로운 cell state로 업데이트

4-4. 출력값 결정

5. GRU
GRU는 LSTM을 단순화시킨 모델로 성능차이는 크지 않다. 별도의 cell state가 존재하지 않고 LSTM의 여러 gate들을 합쳐서 update gate로 합쳐서 단순화 시킴


6. 양방향 RNN -> 역순으로 학습하는 신경망도 있음

7. Attention
지금까지의 모델은 다 이전 은닉 노드들이 버려지고 최종 은닉 노드만이 활용된다는 문제점이 존재했지만, attention을 통해 어느 시점의 정보가 rnn의 최종 출력 값에 영향을 미치는지 알려줄 수 있는 메커니즘이다.

이는 마지막 은닉 노드와 첫 은닉노드부터 계산하여 0애서 1사이의 값을 내고 이 값들을 모두 더하면 1이 나오는 값을 도출한다.

이는 벡터 간의 내적을 통해 유사도가 높으면 큰 값을 내어 softmax에 넣는 방식으로 이루어진다.

또한, 이러한 가중치들을 선형 결합을 통해 context vector를 뽑아내고 이를 에측하는 과정을 거친다.

따라서 이를 정리해보면 이전 은닉 노드들과 마지막 은닉 노드 간의 유사도를 나타내는 가중치를 구하고 이를 더한 다음, 마지막 은닉 노드와 vector concat을 진행하여 tanh(x)에 넣고 이를 다시 softmax를 통해 최종 출력값을 뽑아내는 구조를 가진다.

Attention 모델은 무조건 쓰도록 하자!